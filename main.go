package main

import (
	"context"
	"flag"
	"fmt"
	"log"
	"net"
	"os"
	"path/filepath"
	"sort"
	"strings"
	"time"

	dp "k8s.io/kubelet/pkg/apis/deviceplugin/v1beta1"

	"google.golang.org/grpc"
	"google.golang.org/grpc/credentials/insecure"
)

// Paths are inside the container; we bind-mount the host paths there.
const (
	kubeletSocketName      = "kubelet.sock"
	defaultDevicePluginDir = "/var/lib/kubelet/device-plugins"
	defaultResourceName    = "nvidia.com/gpu"
	defaultPluginSocket    = "nvidia-cdi-device-plugin.sock"

	devGlob = "/dev/nvidia[0-9]*"
)

type NvidiaCDIDevicePlugin struct {
        dp.UnimplementedDevicePluginServer
	resourceName string
	kubeletDir   string
	socketPath   string

	devs map[string]*dp.Device

	server *grpc.Server
	stop   chan struct{}
}

func NewPlugin(resourceName, kubeletDir, socketName string) (*NvidiaCDIDevicePlugin, error) {
	devs, err := discoverDevices(resourceName)
	if err != nil {
		return nil, fmt.Errorf("discover devices: %w", err)
	}

	socketPath := filepath.Join(kubeletDir, socketName)

	return &NvidiaCDIDevicePlugin{
		resourceName: resourceName,
		kubeletDir:   kubeletDir,
		socketPath:   socketPath,
		devs:         devs,
		stop:         make(chan struct{}),
	}, nil
}

// discoverDevices finds /dev/nvidia[0-9]* and maps them to device IDs:
//
//   nvidia.com/gpu=0
//   nvidia.com/gpu=1
//
// These IDs must match the CDI device names generated by nvidia-ctk.
func discoverDevices(resourceName string) (map[string]*dp.Device, error) {
	paths, err := filepath.Glob(devGlob)
	if err != nil {
		return nil, err
	}

	sort.Strings(paths)

	devs := make(map[string]*dp.Device)

	for idx := range paths {
		id := fmt.Sprintf("%s=%d", resourceName, idx)

		devs[id] = &dp.Device{
			ID:     id,
			Health: dp.Healthy,
		}
	}

	if len(devs) == 0 {
		log.Printf("warning: no devices found matching %q", devGlob)
	}

	return devs, nil
}

// ---- DevicePluginServer implementation ----

func (p *NvidiaCDIDevicePlugin) GetDevicePluginOptions(
	ctx context.Context, in *dp.Empty,
) (*dp.DevicePluginOptions, error) {
	return &dp.DevicePluginOptions{
		PreStartRequired:              false,
		GetPreferredAllocationAvailable: false,
	}, nil
}

func (p *NvidiaCDIDevicePlugin) ListAndWatch(
	_ *dp.Empty, stream dp.DevicePlugin_ListAndWatchServer,
) error {
	log.Printf("ListAndWatch: advertising %d devices", len(p.devs))

	devList := make([]*dp.Device, 0, len(p.devs))
	for _, d := range p.devs {
		devList = append(devList, d)
	}

	if err := stream.Send(&dp.ListAndWatchResponse{Devices: devList}); err != nil {
		return err
	}

	// Minimal implementation: just block until stop is closed.
	<-p.stop

	return nil
}

func (p *NvidiaCDIDevicePlugin) Allocate(
	ctx context.Context, req *dp.AllocateRequest,
) (*dp.AllocateResponse, error) {
	resp := &dp.AllocateResponse{
		ContainerResponses: make([]*dp.ContainerAllocateResponse, 0, len(req.ContainerRequests)),
	}

	for _, creq := range req.ContainerRequests {
		var cdiDevices []*dp.CDIDevice

		for _, devID := range creq.DevicesIds {
			if _, ok := p.devs[devID]; !ok {
				return nil, fmt.Errorf("allocate: unknown device ID %q", devID)
			}

			// Device ID is already the fully-qualified CDI device name.
			cdiDevices = append(cdiDevices, &dp.CDIDevice{
				Name: devID,
			})
		}

		cresp := &dp.ContainerAllocateResponse{
			// No envs, mounts, or DeviceSpecs.
			// CDI devices is all we need.
			CdiDevices: cdiDevices,
		}

		resp.ContainerResponses = append(resp.ContainerResponses, cresp)
	}

	return resp, nil
}

func (p *NvidiaCDIDevicePlugin) GetPreferredAllocation(
	ctx context.Context, req *dp.PreferredAllocationRequest,
) (*dp.PreferredAllocationResponse, error) {
	// Simple implementation: just echo back the first N available devices.
	// Kubelet may never call this; it's optional.
	out := &dp.PreferredAllocationResponse{}

	for _, creq := range req.ContainerRequests {
		resp := &dp.ContainerPreferredAllocationResponse{}

		available := creq.AvailableDeviceIDs
		size := int(creq.AllocationSize)
		if size > len(available) {
			size = len(available)
		}

		resp.DeviceIDs = available[:size]
		out.ContainerResponses = append(out.ContainerResponses, resp)
	}

	return out, nil
}

func (p *NvidiaCDIDevicePlugin) PreStartContainer(
	context.Context, *dp.PreStartContainerRequest,
) (*dp.PreStartContainerResponse, error) {
	// Not needed for GPUs here.
	return &dp.PreStartContainerResponse{}, nil
}

// ---- gRPC server lifecycle and registration ----

func (p *NvidiaCDIDevicePlugin) cleanupSocket() {
	if err := os.Remove(p.socketPath); err != nil && !os.IsNotExist(err) {
		log.Printf("warning: remove old socket %s: %v", p.socketPath, err)
	}
}

func (p *NvidiaCDIDevicePlugin) Start() error {
	p.cleanupSocket()

	l, err := net.Listen("unix", p.socketPath)
	if err != nil {
		return fmt.Errorf("listen on %s: %w", p.socketPath, err)
	}

	server := grpc.NewServer()
	dp.RegisterDevicePluginServer(server, p)
	p.server = server

	go func() {
		if err := server.Serve(l); err != nil {
			log.Fatalf("gRPC server crashed: %v", err)
		}
	}()

	// Wait for the server to be up before registering with kubelet.
	// Kubelet expects the endpoint to be reachable.
	connTimeout := 5 * time.Second
	deadline := time.Now().Add(connTimeout)

	for {
		if time.Now().After(deadline) {
			return fmt.Errorf("timeout waiting for gRPC server to start")
		}

		conn, err := grpc.Dial(
			fmt.Sprintf("unix://%s", p.socketPath),
			grpc.WithTransportCredentials(insecure.NewCredentials()),
			grpc.WithBlock(),
			grpc.WithTimeout(1*time.Second),
		)
		if err == nil {
			_ = conn.Close()
			break
		}

		time.Sleep(200 * time.Millisecond)
	}

	return nil
}

func (p *NvidiaCDIDevicePlugin) Stop() {
	close(p.stop)

	if p.server != nil {
		p.server.Stop()
	}
	p.cleanupSocket()
}

func (p *NvidiaCDIDevicePlugin) Register() error {
	kubeletSocket := filepath.Join(p.kubeletDir, kubeletSocketName)

	conn, err := grpc.Dial(
		fmt.Sprintf("unix://%s", kubeletSocket),
		grpc.WithTransportCredentials(insecure.NewCredentials()),
		grpc.WithBlock(),
		grpc.WithTimeout(5*time.Second),
	)
	if err != nil {
		return fmt.Errorf("connect to kubelet at %s: %w", kubeletSocket, err)
	}
	defer conn.Close()

	client := dp.NewRegistrationClient(conn)

	req := &dp.RegisterRequest{
		Version:      dp.Version,
		Endpoint:     filepath.Base(p.socketPath),
		ResourceName: p.resourceName,
		Options: &dp.DevicePluginOptions{
			PreStartRequired:              false,
			GetPreferredAllocationAvailable: false,
		},
	}

	log.Printf("Registering device plugin with kubelet for resource %s", p.resourceName)

	if _, err := client.Register(context.Background(), req); err != nil {
		return fmt.Errorf("register with kubelet: %w", err)
	}

	return nil
}

func main() {
	var (
		resourceName = flag.String("resource-name", defaultResourceName,
			"kubernetes resource name to advertise (must match CDI kind)")
		kubeletDir = flag.String("kubelet-dir", defaultDevicePluginDir,
			"kubelet device plugin directory")
		socketName = flag.String("socket-name", defaultPluginSocket,
			"unix domain socket name for this plugin")
	)
	flag.Parse()

	if !strings.Contains(*resourceName, "/") {
		log.Fatalf("resource-name must be fully qualified, e.g. nvidia.com/gpu")
	}

	plugin, err := NewPlugin(*resourceName, *kubeletDir, *socketName)
	if err != nil {
		log.Fatalf("create plugin: %v", err)
	}

	if err := plugin.Start(); err != nil {
		log.Fatalf("start plugin: %v", err)
	}

	if err := plugin.Register(); err != nil {
		log.Fatalf("register plugin: %v", err)
	}

	log.Printf("nvidia CDI device plugin running. resource=%s", *resourceName)

	// Block forever; in a more robust implementation youâ€™d watch for kubelet restarts
	// and re-register, but this is enough to get you unblocked.
	select {}
}
